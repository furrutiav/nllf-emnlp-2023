{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>ab</th>\n",
       "      <th>q</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does carbon farming provide a cost-effective o...</td>\n",
       "      <td>In this study, we apply a whole farm bioeconom...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Influence of Organic and Mineral Fertilizers o...</td>\n",
       "      <td>The intensive use of mineral (M) fertilizers m...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Change Mitigation Options in the Fores...</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change ...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rye cover crop incorporation and high watertab...</td>\n",
       "      <td>Drainage and cultivation of peat soils almost ...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerging Issues and Potential Opportunities in...</td>\n",
       "      <td>The rice-wheat cropping system (RWCS) is the b...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>Crop-livestock integration provides opportunit...</td>\n",
       "      <td>CONTEXT: The Greater Mekong Subregion has been...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>Pesticide Use and Associated Greenhouse Gas Em...</td>\n",
       "      <td>The production of synthetic pesticides is ener...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>A Review: Soil Management, Sustainable Strateg...</td>\n",
       "      <td>Conservative and sustainable soil management i...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>Agricultural waste recycling in horticultural ...</td>\n",
       "      <td>The vegetables supply chain of intensive farmi...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>Herd parameters in organic and conventional da...</td>\n",
       "      <td>In the study on Climate effects and sustainabi...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1908 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      t  \\\n",
       "0     Does carbon farming provide a cost-effective o...   \n",
       "1     Influence of Organic and Mineral Fertilizers o...   \n",
       "2     Climate Change Mitigation Options in the Fores...   \n",
       "3     Rye cover crop incorporation and high watertab...   \n",
       "4     Emerging Issues and Potential Opportunities in...   \n",
       "...                                                 ...   \n",
       "1903  Crop-livestock integration provides opportunit...   \n",
       "1904  Pesticide Use and Associated Greenhouse Gas Em...   \n",
       "1905  A Review: Soil Management, Sustainable Strateg...   \n",
       "1906  Agricultural waste recycling in horticultural ...   \n",
       "1907  Herd parameters in organic and conventional da...   \n",
       "\n",
       "                                                     ab  \\\n",
       "0     In this study, we apply a whole farm bioeconom...   \n",
       "1     The intensive use of mineral (M) fertilizers m...   \n",
       "2     The Intergovernmental Panel on Climate Change ...   \n",
       "3     Drainage and cultivation of peat soils almost ...   \n",
       "4     The rice-wheat cropping system (RWCS) is the b...   \n",
       "...                                                 ...   \n",
       "1903  CONTEXT: The Greater Mekong Subregion has been...   \n",
       "1904  The production of synthetic pesticides is ener...   \n",
       "1905  Conservative and sustainable soil management i...   \n",
       "1906  The vegetables supply chain of intensive farmi...   \n",
       "1907  In the study on Climate effects and sustainabi...   \n",
       "\n",
       "                                                      q  label  \n",
       "0     Does the article discuss agroecological practi...      1  \n",
       "1     Does the article discuss agroecological practi...      1  \n",
       "2     Does the article discuss agroecological practi...      1  \n",
       "3     Does the article discuss agroecological practi...      1  \n",
       "4     Does the article discuss agroecological practi...      1  \n",
       "...                                                 ...    ...  \n",
       "1903  Does the article discuss the impact of methane...      1  \n",
       "1904  Does the article discuss the impact of methane...      0  \n",
       "1905  Does the article discuss the impact of methane...      0  \n",
       "1906  Does the article discuss the impact of methane...      0  \n",
       "1907  Does the article discuss the impact of methane...      1  \n",
       "\n",
       "[1908 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"train_bsq_abstract_chatgpt_v2.xlsx\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1717, 5), (191, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.sample(frac=0.9, random_state=2023)\n",
    "df_val = df.loc[[ix for ix in df.index if ix not in df_train.index]]\n",
    "\n",
    "df_train = df_train.reset_index()\n",
    "df_val = df_val.reset_index()\n",
    "df_train.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "e = bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'], [100, 102, 0, 101, 103])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.all_special_tokens, bert_tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "12\n",
      "8\n",
      "12\n",
      "9\n",
      "15\n",
      "12\n",
      "7\n",
      "9\n",
      "22\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for s in df[\"q\"].unique():\n",
    "    print(len(bert_tokenizer.tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class DatasetTaskChatGPT(Dataset):\n",
    "    def __init__(self, df, maxlen_ab=484, maxlen_q=25):\n",
    "        self.df = df\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.maxlen_ab = maxlen_ab\n",
    "        self.maxlen_q = maxlen_q\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence2 = str(self.df.loc[index, 'ab'])\n",
    "        sentence3 = str(self.df.loc[index, 'q'])\n",
    "\n",
    "        label = int(self.df.loc[index, \"label\"])\n",
    "        \n",
    "        tokens2 = self.tokenizer.tokenize(sentence2) if len(sentence2)>0 else [\"[UNK]\"]\n",
    "        tokens3 = self.tokenizer.tokenize(sentence3) if len(sentence3)>0 else [\"[UNK]\"]\n",
    "\n",
    "        if len(tokens2) <= self.maxlen_ab:\n",
    "            tokens2 = tokens2 + ['[PAD]' for _ in range(self.maxlen_ab - len(tokens2))]\n",
    "        else:\n",
    "            tokens2 = tokens2[:self.maxlen_ab]\n",
    "        \n",
    "        if len(tokens3) <= self.maxlen_q:\n",
    "            tokens3 = tokens3 + ['[PAD]' for _ in range(self.maxlen_q - len(tokens3))]\n",
    "        else:\n",
    "            tokens3 = tokens3[:self.maxlen_q]\n",
    "          \n",
    "        tokens = [\"[CLS]\"]+tokens2+[\"[SEP]\"]+tokens3+[\"[SEP]\"]\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "        attn_mask = (tokens_ids_tensor != 0).long() # [PAD] => 0\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = DatasetTaskChatGPT(df = df_train)\n",
    "val_set = DatasetTaskChatGPT(df = df_val)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, num_workers = 2, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=16, num_workers = 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        torch.manual_seed(2022)\n",
    "        \n",
    "        self.bert_layer = BertModel.from_pretrained(model_name).cuda()\n",
    "        self.cls_layer = nn.Linear(768, 2).cuda()\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "\n",
    "        cont_reps = self.bert_layer(seq, attention_mask=attn_masks)\n",
    "        cls_rep = cont_reps.last_hidden_state[:, 0]\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Classifier()\n",
    "\n",
    "weights = torch.tensor([2.09, 1.])#torch.tensor([1., 2.188])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, reduction='mean').cuda()\n",
    "\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    soft_probs = probs.argmax(1)\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "    \n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval()\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits, labels).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_precision_recall_fscore_support(net, dataloader):\n",
    "    net.eval()\n",
    "    preds = []\n",
    "    tests = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            soft_probs = probs.argmax(1)\n",
    "            preds += soft_probs.squeeze().tolist()\n",
    "            tests += labels.tolist()\n",
    "    return tests, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, train_loader, val_loader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()  \n",
    "\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % 10 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                # tests, preds = evaluate_precision_recall_fscore_support(net, train_loader)\n",
    "                # print(classification_report(tests, preds))\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Train Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
    "        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
    "        tests, preds = evaluate_precision_recall_fscore_support(net, val_loader)\n",
    "        print(classification_report(tests, preds))\n",
    "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep+1, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 of epoch 1 complete. Loss : 0.00018122400797437876 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 1 complete. Loss : 5.011493340134621e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 1 complete. Loss : 0.00010232042404823005 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 1 complete. Loss : 0.0002768869453575462 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 1 complete. Loss : 0.00021331911557354033 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 1 complete. Loss : 0.00013317404955159873 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 1 complete. Loss : 9.810117626329884e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 1 complete. Loss : 0.00010055983148049563 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 1 complete. Loss : 0.00015900244761724025 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 1 complete. Loss : 0.00011983579315710813 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 1 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.147731490433216\n",
      "Iteration 10 of epoch 2 complete. Loss : 0.00015568672097288072 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 2 complete. Loss : 4.584450289257802e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 2 complete. Loss : 9.140828478848562e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 2 complete. Loss : 0.0002459237293805927 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 2 complete. Loss : 0.00018987676594406366 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 2 complete. Loss : 0.00011612303205765784 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 2 complete. Loss : 8.776086906436831e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 2 complete. Loss : 8.86870693648234e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 2 complete. Loss : 0.00013946802937425673 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 2 complete. Loss : 0.00010456477320985869 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 2 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.171156438688437\n",
      "Iteration 10 of epoch 3 complete. Loss : 0.00013562096864916384 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 3 complete. Loss : 4.214598084217869e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 3 complete. Loss : 8.218170114560053e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 3 complete. Loss : 0.00021998994634486735 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 3 complete. Loss : 0.00017032599134836346 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 3 complete. Loss : 0.00010224992729490623 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 3 complete. Loss : 7.911986176623031e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 3 complete. Loss : 7.889339030953124e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 3 complete. Loss : 0.00012369301111903042 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 3 complete. Loss : 9.21562677831389e-05 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 3 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.1934149463971457\n",
      "Iteration 10 of epoch 4 complete. Loss : 0.00011937534873140976 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 4 complete. Loss : 3.8922513340367004e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 4 complete. Loss : 7.429732795571908e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 4 complete. Loss : 0.0001979893713723868 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 4 complete. Loss : 0.00015369111497420818 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 4 complete. Loss : 9.072024840861559e-05 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 4 complete. Loss : 7.180283864727244e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 4 complete. Loss : 7.069329149089754e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 4 complete. Loss : 0.0001106949130189605 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 4 complete. Loss : 8.193822577595711e-05 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 4 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.2147074192762375\n",
      "Iteration 10 of epoch 5 complete. Loss : 0.00010592651960905641 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 5 complete. Loss : 3.608119368436746e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 5 complete. Loss : 6.744259007973596e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 5 complete. Loss : 0.0001791042013792321 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 5 complete. Loss : 0.0001393940910929814 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 5 complete. Loss : 8.104016887955368e-05 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 5 complete. Loss : 6.546484655700624e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 5 complete. Loss : 6.369673064909875e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 5 complete. Loss : 9.972217958420515e-05 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 5 complete. Loss : 7.333026587730274e-05 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 5 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.2351849203308425\n",
      "Iteration 10 of epoch 6 complete. Loss : 9.461933223064989e-05 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 6 complete. Loss : 3.355805165483616e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 6 complete. Loss : 6.146956729935482e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 6 complete. Loss : 0.00016275765665341169 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 6 complete. Loss : 0.00012695504119619727 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 6 complete. Loss : 7.276413816725835e-05 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 6 complete. Loss : 5.999389759381302e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 6 complete. Loss : 5.768733535660431e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 6 complete. Loss : 9.037068957695737e-05 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 6 complete. Loss : 6.603863585041836e-05 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 6 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.254968104263147\n",
      "Iteration 10 of epoch 7 complete. Loss : 8.504799188813195e-05 Train Accuracy : 1.0\n",
      "Iteration 20 of epoch 7 complete. Loss : 3.128426760667935e-05 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 7 complete. Loss : 5.6243425206048414e-05 Train Accuracy : 1.0\n",
      "Iteration 40 of epoch 7 complete. Loss : 0.0001484493986936286 Train Accuracy : 1.0\n",
      "Iteration 50 of epoch 7 complete. Loss : 0.00011602570884861052 Train Accuracy : 1.0\n",
      "Iteration 60 of epoch 7 complete. Loss : 6.565348303411156e-05 Train Accuracy : 1.0\n",
      "Iteration 70 of epoch 7 complete. Loss : 5.517843965208158e-05 Train Accuracy : 1.0\n",
      "Iteration 80 of epoch 7 complete. Loss : 5.2491970564005896e-05 Train Accuracy : 1.0\n",
      "Iteration 90 of epoch 7 complete. Loss : 8.22589427116327e-05 Train Accuracy : 1.0\n",
      "Iteration 100 of epoch 7 complete. Loss : 5.973876250209287e-05 Train Accuracy : 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Epoch 7 complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.274153307080269\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "train(net, criterion, opti, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       542\n",
      "           1       1.00      1.00      1.00      1175\n",
      "\n",
      "    accuracy                           1.00      1717\n",
      "   macro avg       1.00      1.00      1.00      1717\n",
      "weighted avg       1.00      1.00      1.00      1717\n",
      "\n",
      "Complete! Train Accuracy : 1.0, Train Loss : 9.452948621334076e-05\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = evaluate(net, criterion, train_loader)\n",
    "tests, preds = evaluate_precision_recall_fscore_support(net, train_loader)\n",
    "print(classification_report(tests, preds))\n",
    "print(\"Complete! Train Accuracy : {}, Train Loss : {}\".format(train_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.52      0.60        75\n",
      "           1       0.74      0.86      0.79       116\n",
      "\n",
      "    accuracy                           0.73       191\n",
      "   macro avg       0.72      0.69      0.70       191\n",
      "weighted avg       0.73      0.73      0.72       191\n",
      "\n",
      "Complete! Validation Accuracy : 0.7284722328186035, Validation Loss : 2.274153307080269\n"
     ]
    }
   ],
   "source": [
    "val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
    "tests, preds = evaluate_precision_recall_fscore_support(net, val_loader)\n",
    "print(classification_report(tests, preds))\n",
    "print(\"Complete! Validation Accuracy : {}, Validation Loss : {}\".format(val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"bert_ft_binary_chatgpt_new\"\n",
    "net.bert_layer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.cls_layer, \"cls_layer.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
