{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>ab</th>\n",
       "      <th>q</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does carbon farming provide a cost-effective o...</td>\n",
       "      <td>In this study, we apply a whole farm bioeconom...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Influence of Organic and Mineral Fertilizers o...</td>\n",
       "      <td>The intensive use of mineral (M) fertilizers m...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Change Mitigation Options in the Fores...</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change ...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rye cover crop incorporation and high watertab...</td>\n",
       "      <td>Drainage and cultivation of peat soils almost ...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerging Issues and Potential Opportunities in...</td>\n",
       "      <td>The rice-wheat cropping system (RWCS) is the b...</td>\n",
       "      <td>Does the article discuss agroecological practi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Consequences of field N2O emissions for the en...</td>\n",
       "      <td>One way of reducing the emissions of fossil fu...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Estimation of net greenhouse gas balance using...</td>\n",
       "      <td>The net greenhouse gas balance (NGHGB), estima...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Farmers' adaptation to climate-smart agricultu...</td>\n",
       "      <td>Some of the measures to be taken to reduce gre...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Valorization Methodology for Agriculture Secto...</td>\n",
       "      <td>Agriculture sector holds an essential role in ...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Maintaining rice production while mitigating m...</td>\n",
       "      <td>China is the largest rice producing and consum...</td>\n",
       "      <td>Does the article discuss the impact of methane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     t  \\\n",
       "0    Does carbon farming provide a cost-effective o...   \n",
       "1    Influence of Organic and Mineral Fertilizers o...   \n",
       "2    Climate Change Mitigation Options in the Fores...   \n",
       "3    Rye cover crop incorporation and high watertab...   \n",
       "4    Emerging Issues and Potential Opportunities in...   \n",
       "..                                                 ...   \n",
       "995  Consequences of field N2O emissions for the en...   \n",
       "996  Estimation of net greenhouse gas balance using...   \n",
       "997  Farmers' adaptation to climate-smart agricultu...   \n",
       "998  Valorization Methodology for Agriculture Secto...   \n",
       "999  Maintaining rice production while mitigating m...   \n",
       "\n",
       "                                                    ab  \\\n",
       "0    In this study, we apply a whole farm bioeconom...   \n",
       "1    The intensive use of mineral (M) fertilizers m...   \n",
       "2    The Intergovernmental Panel on Climate Change ...   \n",
       "3    Drainage and cultivation of peat soils almost ...   \n",
       "4    The rice-wheat cropping system (RWCS) is the b...   \n",
       "..                                                 ...   \n",
       "995  One way of reducing the emissions of fossil fu...   \n",
       "996  The net greenhouse gas balance (NGHGB), estima...   \n",
       "997  Some of the measures to be taken to reduce gre...   \n",
       "998  Agriculture sector holds an essential role in ...   \n",
       "999  China is the largest rice producing and consum...   \n",
       "\n",
       "                                                     q  label  \n",
       "0    Does the article discuss agroecological practi...      1  \n",
       "1    Does the article discuss agroecological practi...      1  \n",
       "2    Does the article discuss agroecological practi...      1  \n",
       "3    Does the article discuss agroecological practi...      1  \n",
       "4    Does the article discuss agroecological practi...      1  \n",
       "..                                                 ...    ...  \n",
       "995  Does the article discuss the impact of methane...      0  \n",
       "996  Does the article discuss the impact of methane...      0  \n",
       "997  Does the article discuss the impact of methane...      0  \n",
       "998  Does the article discuss the impact of methane...      1  \n",
       "999  Does the article discuss the impact of methane...      1  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"train_bsq_abstract_chatgpt.xlsx\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.9, random_state=2023)\n",
    "df_val = df.loc[[ix for ix in df.index if ix not in df_train.index]]\n",
    "\n",
    "df_train = df_train.reset_index()\n",
    "df_val = df_val.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "e = bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'], [100, 102, 0, 101, 103])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.all_special_tokens, bert_tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n",
      "12\n",
      "15\n",
      "12\n",
      "7\n",
      "9\n",
      "22\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for s in df[\"q\"].unique():\n",
    "    print(len(bert_tokenizer.tokenize(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class DatasetTaskChatGPT(Dataset):\n",
    "    def __init__(self, df, maxlen_t=0, maxlen_ab=483, maxlen_q=25):\n",
    "        self.df = df\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.maxlen_t = maxlen_t\n",
    "        self.maxlen_ab = maxlen_ab\n",
    "        self.maxlen_q = maxlen_q\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence1 = str(self.df.loc[index, 't'])\n",
    "        sentence2 = str(self.df.loc[index, 'ab'])\n",
    "        sentence3 = str(self.df.loc[index, 'q'])\n",
    "\n",
    "        label = int(self.df.loc[index, \"label\"])\n",
    "        \n",
    "        tokens1 = self.tokenizer.tokenize(sentence1) if len(sentence1)>0 else [\"[UNK]\"]\n",
    "        tokens2 = self.tokenizer.tokenize(sentence2) if len(sentence2)>0 else [\"[UNK]\"]\n",
    "        tokens3 = self.tokenizer.tokenize(sentence3) if len(sentence3)>0 else [\"[UNK]\"]\n",
    "\n",
    "        if len(tokens1) <= self.maxlen_t:\n",
    "            tokens1 = tokens1 + ['[PAD]' for _ in range(self.maxlen_t - len(tokens1))]\n",
    "        else:\n",
    "            tokens1 = tokens1[:self.maxlen_t]\n",
    "\n",
    "        if len(tokens2) <= self.maxlen_ab:\n",
    "            tokens2 = tokens2 + ['[PAD]' for _ in range(self.maxlen_ab - len(tokens2))]\n",
    "        else:\n",
    "            tokens2 = tokens2[:self.maxlen_ab]\n",
    "        \n",
    "        if len(tokens3) <= self.maxlen_q:\n",
    "            tokens3 = tokens3 + ['[PAD]' for _ in range(self.maxlen_q - len(tokens3))]\n",
    "        else:\n",
    "            tokens3 = tokens3[:self.maxlen_q]\n",
    "          \n",
    "        tokens = [\"[CLS]\"]+tokens1+[\"[SEP]\"]+tokens2+[\"[SEP]\"]+tokens3+[\"[SEP]\"]\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
    "        attn_mask = (tokens_ids_tensor != 0).long() # [PAD] => 0\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = DatasetTaskChatGPT(df = df_train)\n",
    "val_set = DatasetTaskChatGPT(df = df_val)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, num_workers = 2, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=16, num_workers = 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        torch.manual_seed(2022)\n",
    "        \n",
    "        self.bert_layer = BertModel.from_pretrained(model_name).cuda()\n",
    "        self.cls_layer = nn.Linear(768, 2).cuda()\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "\n",
    "        cont_reps = self.bert_layer(seq, attention_mask=attn_masks)\n",
    "        cls_rep = cont_reps.last_hidden_state[:, 0]\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Classifier()\n",
    "\n",
    "weights = torch.tensor([1., 1.])#torch.tensor([1., 2.188])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights, reduction='mean').cuda()\n",
    "\n",
    "opti = optim.Adam(net.parameters(), lr = 8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    soft_probs = probs.argmax(1)\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "    \n",
    "def evaluate(net, criterion, dataloader):\n",
    "    net.eval()\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits, labels).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_precision_recall_fscore_support(net, dataloader):\n",
    "    net.eval()\n",
    "    preds = []\n",
    "    tests = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "            logits = net(seq, attn_masks)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            soft_probs = probs.argmax(1)\n",
    "            preds += soft_probs.squeeze().tolist()\n",
    "            tests += labels.tolist()\n",
    "    return tests, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, train_loader, val_loader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            opti.zero_grad()  \n",
    "\n",
    "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
    "\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            opti.step()\n",
    "\n",
    "            if (it + 1) % 10 == 0:\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                # tests, preds = evaluate_precision_recall_fscore_support(net, train_loader)\n",
    "                # print(classification_report(tests, preds))\n",
    "                print(\"Iteration {} of epoch {} complete. Loss : {} Train Accuracy : {}\".format(it+1, ep+1, loss.item(), acc))\n",
    "        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
    "        tests, preds = evaluate_precision_recall_fscore_support(net, val_loader)\n",
    "        print(classification_report(tests, preds))\n",
    "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep+1, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 of epoch 1 complete. Loss : 0.28662875294685364 Train Accuracy : 0.875\n",
      "Iteration 20 of epoch 1 complete. Loss : 0.2613835632801056 Train Accuracy : 0.875\n",
      "Iteration 30 of epoch 1 complete. Loss : 0.3063660264015198 Train Accuracy : 0.8125\n",
      "Iteration 40 of epoch 1 complete. Loss : 0.5068246126174927 Train Accuracy : 0.625\n",
      "Iteration 50 of epoch 1 complete. Loss : 0.45571690797805786 Train Accuracy : 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61        35\n",
      "           1       0.79      0.77      0.78        65\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.69      0.70      0.70       100\n",
      "weighted avg       0.72      0.72      0.72       100\n",
      "\n",
      "Epoch 1 complete! Validation Accuracy : 0.6964285969734192, Validation Loss : 0.6241320861237389\n",
      "Iteration 10 of epoch 2 complete. Loss : 0.23604127764701843 Train Accuracy : 0.875\n",
      "Iteration 20 of epoch 2 complete. Loss : 0.26598864793777466 Train Accuracy : 0.9375\n",
      "Iteration 30 of epoch 2 complete. Loss : 0.14364948868751526 Train Accuracy : 0.9375\n",
      "Iteration 40 of epoch 2 complete. Loss : 0.5933514833450317 Train Accuracy : 0.6875\n",
      "Iteration 50 of epoch 2 complete. Loss : 0.419561505317688 Train Accuracy : 0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.80      0.67        35\n",
      "           1       0.86      0.68      0.76        65\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.72      0.74      0.71       100\n",
      "weighted avg       0.76      0.72      0.73       100\n",
      "\n",
      "Epoch 2 complete! Validation Accuracy : 0.6964285969734192, Validation Loss : 0.64494760121618\n",
      "Iteration 10 of epoch 3 complete. Loss : 0.14230313897132874 Train Accuracy : 0.9375\n",
      "Iteration 20 of epoch 3 complete. Loss : 0.17180542647838593 Train Accuracy : 0.9375\n",
      "Iteration 30 of epoch 3 complete. Loss : 0.28348153829574585 Train Accuracy : 0.875\n",
      "Iteration 40 of epoch 3 complete. Loss : 0.5240004062652588 Train Accuracy : 0.5\n",
      "Iteration 50 of epoch 3 complete. Loss : 0.427479088306427 Train Accuracy : 0.875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.67        35\n",
      "           1       0.83      0.77      0.80        65\n",
      "\n",
      "    accuracy                           0.75       100\n",
      "   macro avg       0.73      0.74      0.73       100\n",
      "weighted avg       0.76      0.75      0.75       100\n",
      "\n",
      "Epoch 3 complete! Validation Accuracy : 0.723214328289032, Validation Loss : 0.6543688092912946\n",
      "Iteration 10 of epoch 4 complete. Loss : 0.21560105681419373 Train Accuracy : 0.9375\n",
      "Iteration 20 of epoch 4 complete. Loss : 0.180596262216568 Train Accuracy : 0.9375\n",
      "Iteration 30 of epoch 4 complete. Loss : 0.20278483629226685 Train Accuracy : 0.875\n",
      "Iteration 40 of epoch 4 complete. Loss : 0.41847896575927734 Train Accuracy : 0.6875\n",
      "Iteration 50 of epoch 4 complete. Loss : 0.42863190174102783 Train Accuracy : 0.875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.80      0.68        35\n",
      "           1       0.87      0.71      0.78        65\n",
      "\n",
      "    accuracy                           0.74       100\n",
      "   macro avg       0.73      0.75      0.73       100\n",
      "weighted avg       0.77      0.74      0.75       100\n",
      "\n",
      "Epoch 4 complete! Validation Accuracy : 0.7142857313156128, Validation Loss : 0.6386651567050389\n",
      "Iteration 10 of epoch 5 complete. Loss : 0.2537257969379425 Train Accuracy : 0.875\n",
      "Iteration 20 of epoch 5 complete. Loss : 0.10098113119602203 Train Accuracy : 0.9375\n",
      "Iteration 30 of epoch 5 complete. Loss : 0.247587651014328 Train Accuracy : 0.8125\n",
      "Iteration 40 of epoch 5 complete. Loss : 0.39472848176956177 Train Accuracy : 0.75\n",
      "Iteration 50 of epoch 5 complete. Loss : 0.29631420969963074 Train Accuracy : 0.9375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.71      0.66        35\n",
      "           1       0.83      0.75      0.79        65\n",
      "\n",
      "    accuracy                           0.74       100\n",
      "   macro avg       0.72      0.73      0.72       100\n",
      "weighted avg       0.75      0.74      0.74       100\n",
      "\n",
      "Epoch 5 complete! Validation Accuracy : 0.7142857313156128, Validation Loss : 0.6063361934253148\n",
      "Iteration 10 of epoch 6 complete. Loss : 0.12466266006231308 Train Accuracy : 0.9375\n",
      "Iteration 20 of epoch 6 complete. Loss : 0.06033257767558098 Train Accuracy : 1.0\n",
      "Iteration 30 of epoch 6 complete. Loss : 0.07823582738637924 Train Accuracy : 0.9375\n",
      "Iteration 40 of epoch 6 complete. Loss : 0.5735083818435669 Train Accuracy : 0.6875\n",
      "Iteration 50 of epoch 6 complete. Loss : 0.4060658812522888 Train Accuracy : 0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69        35\n",
      "           1       0.86      0.75      0.80        65\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.74      0.76      0.75       100\n",
      "weighted avg       0.78      0.76      0.76       100\n",
      "\n",
      "Epoch 6 complete! Validation Accuracy : 0.7321428656578064, Validation Loss : 0.552622737629073\n",
      "Iteration 10 of epoch 7 complete. Loss : 0.11214420199394226 Train Accuracy : 0.9375\n",
      "Iteration 20 of epoch 7 complete. Loss : 0.19165275990962982 Train Accuracy : 0.9375\n",
      "Iteration 30 of epoch 7 complete. Loss : 0.0910615473985672 Train Accuracy : 0.9375\n",
      "Iteration 40 of epoch 7 complete. Loss : 0.25582173466682434 Train Accuracy : 0.8125\n",
      "Iteration 50 of epoch 7 complete. Loss : 0.4940092861652374 Train Accuracy : 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74        35\n",
      "           1       0.91      0.75      0.82        65\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.78      0.81      0.78       100\n",
      "weighted avg       0.82      0.79      0.79       100\n",
      "\n",
      "Epoch 7 complete! Validation Accuracy : 0.7589285969734192, Validation Loss : 0.6366780655724662\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "train(net, criterion, opti, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       336\n",
      "           1       0.94      0.88      0.91       564\n",
      "\n",
      "    accuracy                           0.89       900\n",
      "   macro avg       0.88      0.89      0.88       900\n",
      "weighted avg       0.89      0.89      0.89       900\n",
      "\n",
      "Complete! Train Accuracy : 0.8892543911933899, Train Loss : 0.26945389192878155\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = evaluate(net, criterion, train_loader)\n",
    "tests, preds = evaluate_precision_recall_fscore_support(net, train_loader)\n",
    "print(classification_report(tests, preds))\n",
    "print(\"Complete! Train Accuracy : {}, Train Loss : {}\".format(train_acc, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74        35\n",
      "           1       0.91      0.75      0.82        65\n",
      "\n",
      "    accuracy                           0.79       100\n",
      "   macro avg       0.78      0.81      0.78       100\n",
      "weighted avg       0.82      0.79      0.79       100\n",
      "\n",
      "Complete! Validation Accuracy : 0.7589285969734192, Validation Loss : 0.6366780655724662\n"
     ]
    }
   ],
   "source": [
    "val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
    "tests, preds = evaluate_precision_recall_fscore_support(net, val_loader)\n",
    "print(classification_report(tests, preds))\n",
    "print(\"Complete! Validation Accuracy : {}, Validation Loss : {}\".format(val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"bert_ft_binary_chatgpt\"\n",
    "net.bert_layer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set.tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.cls_layer, \"cls_layer.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
