{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from class_sub_task_labelisation import SubTaskLabelisator\n",
    "from class_nllfg_training import NLLFGeneratorTraining\n",
    "from class_nllf_generation import NLLFGeneratorInAction\n",
    "from class_nllfg_integration import NLLFIntergration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plug and play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Zero-shot Sub-task Labelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a labelisator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelisator = SubTaskLabelisator(\n",
    "    api_key = \"<OPENAI-API-KEY>\",                   # OpenAI - API Key: https://platform.openai.com/account/api-keys\n",
    "    file_name_dict_bsqs = \"data/dict_bsqs.json\",    # File name of your JSON with BSQs\n",
    "    file_name_data_train = \"data/data_train.xlsx\",  # File name of your .xlsx Training Dataset\n",
    "    sentence_col_name = \"abstract\",                 # Column name of your text-to-classify\n",
    "    sample_size = 100,                              # Sample size for zero-shot labelisation: Integer number or fraction between 0 to 1\n",
    "    seed = 42                                       # Random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the labelisator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "labelisator.run_labeling(\n",
    "    root_labels = \"01_labels\",  # Root folder to save the weak-labels\n",
    "    temp=0,                     # Temperature of GPT-3.5-turbo\n",
    "    max_t=5,                    # Max. number of output-tokens\n",
    "    verbose=True                # Print status\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training of NLLF Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = NLLFGeneratorTraining(\n",
    "    file_name_dict_bsqs = \"data/dict_bsqs.json\",    # File name of your JSON with BSQs\n",
    "    root_labels = \"01_labels\",                      # Root folder of the weak-labels\n",
    "    sentence_col_name = \"abstract\",                 # Column name of your text-to-classify\n",
    "    model_name = \"bert-base-uncased\",               # Base model name for your NLLF generator (This version: Only for BERT models from HuggingFace)\n",
    "    maxlen_s=489,                                   # Max. number of tokens for your tokenize text-to-classify\n",
    "    maxlen_bsq=20,                                  # Max. number of tokens for your tokenize BSQs\n",
    "    batch_size=8                                    # Batch size for the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.train(\n",
    "    epochs=5,       # Number of epochs for the training\n",
    "    lr=2e-5,        # Learning rate for the training\n",
    "    verbose=True    # Print status\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "training.save(\n",
    "    hf_token = \"<HF-TOKEN>\",    # Hugging Face User Access Token: https://huggingface.co/settings/tokens\n",
    "    repo_name = \"example_juke\", # Repo. name for your NLLF generator\n",
    "    username= \"<HF-USERNAME>\"   # Hugging Face Username\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: NLLF Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "generator = NLLFGeneratorInAction(\n",
    "    file_name_new_dict_bsqs = \"data/new_dict_bsqs.json\",    # File name of your JSON with new BSQs\n",
    "    maxlen_s = 489,                                         # Max. number of tokens for your tokenize text-to-classify\n",
    "    maxlen_bsq = 20,                                        # Max. number of tokens for your tokenize BSQs\n",
    "    username = \"<HF-USERNAME>\",                             # Hugging Face Username\n",
    "    repo_name = \"example_juke\",                             # Repo. name for your NLLF generator\n",
    "    file_name_data_train = \"data/data_train.xlsx\",          # File name of your .xlsx Training Dataset\n",
    "    file_name_data_val = \"data/data_val.xlsx\",              # File name of your .xlsx Validation Dataset\n",
    "    file_name_data_test = \"data/data_test.xlsx\",            # File name of your .xlsx Testing Dataset\n",
    "    sentence_col_name = \"abstract\"                          # Column name of your text-to-classify\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "generator.apply(\n",
    "    root_labels=\"02_labels\",    # Root folder to save the NLL (Natural Language Learned) features\n",
    "    verbose=True                # Print status\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: NLLF Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator = NLLFIntergration(\n",
    "    root_labels = \"02_labels\",              # Root folder of the NLL (Natural Language Learned) features\n",
    "    file_name_support = \"data/support.txt\", # File name of your .txt with support NLLF\n",
    "    label_col_name = \"label\",               # Column name of your task-label\n",
    "    dt_max_depth=5                          # Max. depth for the Decision Tree (DT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save DT and predictions of the integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "integrator.save_predict(\n",
    "    root_labels = \"03_model_predictions\"    # Root folder for predictions and model parameters\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
